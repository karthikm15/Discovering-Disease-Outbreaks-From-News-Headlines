{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Extracting Real-Time Headlines from News Channels</h1>\n",
    "\n",
    "<p>Extracting headlines from the top 100 news websites, this code aims to provide an accurate and real-time list of relevant headlines pertaining to coronavirus outbreaks. A detailed step-by-step explanation of the code is shown below:</p>\n",
    "\n",
    "<h2>Step 1: Import Libraries</h2>\n",
    "\n",
    "Each of the libraries imported provides a sizable contribution to the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting boto3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/f4/41c1d8a69b07b2a087a7e552cbed21111ff36706fec2f1ba9983fba95771/boto3-1.14.20-py2.py3-none-any.whl (128kB)\n",
      "\u001b[K    100% |████████████████████████████████| 133kB 2.1MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting botocore<1.18.0,>=1.17.20 (from boto3)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/a6/1710181d97a6763ccced7f69fff8beea751633af2a101c3d02826cf4acce/botocore-1.17.20-py2.py3-none-any.whl (6.3MB)\n",
      "\u001b[K    100% |████████████████████████████████| 6.3MB 1.2MB/s eta 0:00:01    65% |█████████████████████           | 4.1MB 23.0MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting s3transfer<0.4.0,>=0.3.0 (from boto3)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/79/e6afb3d8b0b4e96cefbdc690f741d7dd24547ff1f94240c997a26fa908d3/s3transfer-0.3.3-py2.py3-none-any.whl (69kB)\n",
      "\u001b[K    100% |████████████████████████████████| 71kB 14.0MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1 (from boto3)\n",
      "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: urllib3<1.26,>=1.20; python_version != \"3.4\" in /Users/karthikmittal/anaconda3/lib/python3.7/site-packages (from botocore<1.18.0,>=1.17.20->boto3) (1.24.1)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /Users/karthikmittal/anaconda3/lib/python3.7/site-packages (from botocore<1.18.0,>=1.17.20->boto3) (0.14)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /Users/karthikmittal/anaconda3/lib/python3.7/site-packages (from botocore<1.18.0,>=1.17.20->boto3) (2.8.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/karthikmittal/anaconda3/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.18.0,>=1.17.20->boto3) (1.12.0)\n",
      "Installing collected packages: jmespath, botocore, s3transfer, boto3\n",
      "Successfully installed boto3-1.14.20 botocore-1.17.20 jmespath-0.10.0 s3transfer-0.3.3\n"
     ]
    }
   ],
   "source": [
    "!pip install boto3\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import urllib.request\n",
    "from django.core.validators import URLValidator\n",
    "from django.core.exceptions import ValidationError\n",
    "import time\n",
    "from datetime import datetime\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Step 2: Initialize and Empty the S3 Bucket</h2>\n",
    "\n",
    "Using the boto3 library and the AWS account configuration credentials, an object is cleared and put into an S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource(\n",
    "    's3',\n",
    "    region_name='us-east-1',\n",
    "    aws_access_key_id=*HIDDEN*,\n",
    "    aws_secret_access_key=*HIDDEN*\n",
    ")\n",
    "\n",
    "content = \"\"\n",
    "s3.Object('headlines', 'headline.txt').put(Body=content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Step 3: Create a Function that Extracts Websites</h2>\n",
    "\n",
    "<p> First, using the urllib library, the code opens the URL and converts it into HTML. Next, utilizing the Beautiful Soup library, the HTML is parsed, and hyperlinks are extracted from the HTML code. Implementing the Django library, the hyperlink URLs are validated, filtered based on the exception URL, and added to an array. This array of URLs is then returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractWebsites(url, exceptionURL):\n",
    "    try:\n",
    "        webUrl = urllib.request.urlopen(url)\n",
    "        data = webUrl.read()\n",
    "        soup = bs(data)\n",
    "        arr = []\n",
    "        for link in soup.find_all('a'):\n",
    "            validate = URLValidator()\n",
    "            href = link.get('href')\n",
    "            if (href != None):\n",
    "                try:\n",
    "                    validate(href)\n",
    "                    if (href.find(exceptionURL) == -1):\n",
    "                        arr.append(href)\n",
    "                except ValidationError as exception:\n",
    "                    continue\n",
    "        return arr\n",
    "    except:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Step 4: Create a Function that Searches a URL for Specific Key Words</h2>\n",
    "\n",
    "The function iterates through the array of key words, and returns true if any of the key words are within the URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_array(url, arrOfPoss):\n",
    "    for index in arrOfPoss:\n",
    "        if (url.find(index.capitalize())!=-1 or url.find(index.lower())!=-1):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Step 5: Extract the Headlines</h2>\n",
    "\n",
    "First, an array of news channels are selected from Feedspot's top 100 news websites. Then, that array is iterated through using the extractWebsites function to find pertinent websites that pertain to COVID-19. \"Interactive\" is chosen as the exceptionURL, as these do not provide relevant headlines. Other headlines are filtered using the search_array function with a set of COVID-19 key words. These filtered headlines are stored in a new array called covid_urls.\n",
    "\n",
    "Using the extractWebsites function again, covid_urls are iterated through to find more COVID-related websites. To reduce runtime, a counter is implemented that stops if there are an excessive amount of websites to run through. Additionally, if this procedure takes longer than ten minutes, the inner for loop will terminate. Once again, \"interactive\" is chosen as the execptionURL, and the headlines are filtered using the search_array function with the same set of COVID-19 key words. These filtered headlines are stored in a new array called new_covid_urls.\n",
    "\n",
    "This process is repeated one more time to optimize the amount of relevant headlines extracted. The filtered headlines are stored in another array called newest_covid_urls. To store the relevant headlines in a text file called \"headline.txt\", the URLs inside new_covid_urls and newest_covid_urls were iterated through. For each website in these arrays, the URL was converted to HTML to determine the title of the URL. Once checked for repetition in the headlines array, the title was added inside the S3 object and the headlines array. The final headlines array was put inside the S3 object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in data:\n",
    "    if (index.find('npr')!=-1 or index.find('NPR')!=-1):\n",
    "        continue\n",
    "    new_data = extractWebsites(index, 'interactive')\n",
    "    covid_urls = []\n",
    "    arr_keywords = ['coronavirus', 'COVID', 'covid', 'pandemic', 'epidemic', 'disease', 'SARS', 'sars', 'virus']\n",
    "    for index1 in new_data:\n",
    "        if (search_array(index1, arr_keywords)):\n",
    "            covid_urls.append(index1)\n",
    "    count = 0\n",
    "    for val in covid_urls:\n",
    "        if count > 13:\n",
    "            break\n",
    "        future = time.time() + 600\n",
    "        newer_data = extractWebsites(val, \"interactive\")\n",
    "        new_covid_urls = []\n",
    "        for index1 in newer_data:\n",
    "            if (search_array(index1, arr_keywords)):\n",
    "                new_covid_urls.append(index1)\n",
    "        for value in new_covid_urls:\n",
    "            if (time.time()) > future:\n",
    "                break\n",
    "            newest_data = extractWebsites(value, \"interactive\")\n",
    "            newest_covid_urls = []\n",
    "            for index2 in newest_data:\n",
    "                if (time.time()) > future:\n",
    "                    break\n",
    "                if (search_array(index2, arr_keywords)):\n",
    "                    newest_covid_urls.append(index2)\n",
    "            for urls1 in newest_covid_urls:\n",
    "                if ((time.time()) > future):\n",
    "                    break\n",
    "                try:\n",
    "                    webUrl = urllib.request.urlopen(urls1)\n",
    "                    data = webUrl.read()\n",
    "                    soup = bs(data)\n",
    "                    try:\n",
    "                        title = soup.find('title').string\n",
    "                        if (title not in headlines) and (search_array(title, arr_keywords)):\n",
    "                            headlines.append(title)\n",
    "                            content = \"\\n\".join(headlines)\n",
    "                            s3.Object('headlines', 'headline.txt').put(Body=content)\n",
    "                    except TypeError as exception:\n",
    "                        print(\"Exception occured\")\n",
    "                        continue\n",
    "                except:\n",
    "                    continue\n",
    "        for urls in new_covid_urls:\n",
    "            if (time.time()) > future:\n",
    "                break\n",
    "            try:\n",
    "                webUrl = urllib.request.urlopen(urls)\n",
    "                data = webUrl.read()\n",
    "                soup = bs(data)\n",
    "                try:\n",
    "                    title = soup.find('title').string\n",
    "                    if (title not in headlines) and (search_array(title, arr_keywords)):\n",
    "                        headlines.append(title)\n",
    "                        content = \"\\n\".join(headlines)\n",
    "                        s3.Object('headlines', 'headline.txt').put(Body=content)\n",
    "                        \n",
    "                except:\n",
    "                    continue\n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "content = \"\\n\".join(headlines)\n",
    "s3.Object('headlines', 'headline.txt').put(Body=content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
